{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mctest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XngDsYz_gkPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aada6a0c-f440-4dc5-f68c-323c5d14d53c"
      },
      "source": [
        "# from google.colab import drive\n",
        "!wget -q https://www.dropbox.com/s/bew0mp12w3eqss8/MCTest.zip\n",
        "!unzip -o /content/MCTest.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/MCTest.zip\n",
            "  inflating: __MACOSX/._MCTest       \n",
            "  inflating: MCTest/.DS_Store        \n",
            "  inflating: __MACOSX/MCTest/._.DS_Store  \n",
            "  inflating: __MACOSX/MCTest/._MCTest  \n",
            "  inflating: __MACOSX/MCTest/._MCTestAnswers  \n",
            "  inflating: MCTest/MCTest/mc160.train.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc160.train.txt  \n",
            "  inflating: MCTest/MCTest/mc160.dev.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc160.dev.txt  \n",
            "  inflating: MCTest/MCTest/mc160.test.tsv  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc160.test.tsv  \n",
            "  inflating: MCTest/MCTest/mc160.train.ans  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc160.train.ans  \n",
            "  inflating: MCTest/MCTest/mc500.train.tsv  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc500.train.tsv  \n",
            "  inflating: MCTest/MCTest/mc160.dev.ans  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc160.dev.ans  \n",
            "  inflating: MCTest/MCTest/mc160.test.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc160.test.txt  \n",
            "  inflating: MCTest/MCTest/mc500.train.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc500.train.txt  \n",
            "  inflating: MCTest/MCTest/mc160.train.tsv  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc160.train.tsv  \n",
            "  inflating: MCTest/MCTest/mc500.train.ans  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc500.train.ans  \n",
            "  inflating: MCTest/MCTest/mc160.dev.tsv  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc160.dev.tsv  \n",
            "  inflating: MCTest/MCTest/LICENSE.pdf  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._LICENSE.pdf  \n",
            "  inflating: MCTest/MCTest/mc500.test.tsv  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc500.test.tsv  \n",
            "  inflating: MCTest/MCTest/mc500.dev.tsv  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc500.dev.tsv  \n",
            "  inflating: MCTest/MCTest/README.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._README.txt  \n",
            "  inflating: MCTest/MCTest/mc500.dev.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc500.dev.txt  \n",
            "  inflating: MCTest/MCTest/mc500.test.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc500.test.txt  \n",
            "  inflating: MCTest/MCTest/mc500.dev.ans  \n",
            "  inflating: __MACOSX/MCTest/MCTest/._mc500.dev.ans  \n",
            "  inflating: MCTest/MCTestAnswers/mc160.test.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTestAnswers/._mc160.test.txt  \n",
            "  inflating: MCTest/MCTestAnswers/mc160.test.ans  \n",
            "  inflating: __MACOSX/MCTest/MCTestAnswers/._mc160.test.ans  \n",
            "  inflating: MCTest/MCTestAnswers/LICENSE.pdf  \n",
            "  inflating: __MACOSX/MCTest/MCTestAnswers/._LICENSE.pdf  \n",
            "  inflating: MCTest/MCTestAnswers/README.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTestAnswers/._README.txt  \n",
            "  inflating: MCTest/MCTestAnswers/mc500.test.ans  \n",
            "  inflating: __MACOSX/MCTest/MCTestAnswers/._mc500.test.ans  \n",
            "  inflating: MCTest/MCTestAnswers/mc500.test.txt  \n",
            "  inflating: __MACOSX/MCTest/MCTestAnswers/._mc500.test.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v50BARd90OaJ"
      },
      "source": [
        "## Import lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTg2L2GQutJA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdlJiwaBrYpn",
        "outputId": "c8d13c16-d1c3-4844-e141-cdacea05313b"
      },
      "source": [
        "! git clone https://github.com/NVIDIA/apex.git\n",
        "% cd apex\n",
        "! pip install -v --no-cache-dir ./\n",
        "%cd ..\n",
        "\n",
        "from apex import amp\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import json\n",
        "\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "from transformers import BertTokenizer, BertForMultipleChoice, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForMultipleChoice\n",
        "from transformers import AlbertConfig, AlbertTokenizer, AlbertForMultipleChoice\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.optim.adam import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/content/apex\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-ij6ne1gy\n",
            "Created temporary directory: /tmp/pip-req-tracker-yzg_yeyt\n",
            "Created requirements tracker '/tmp/pip-req-tracker-yzg_yeyt'\n",
            "Created temporary directory: /tmp/pip-install-lmptia6q\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-54p4ej83\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-yzg_yeyt'\n",
            "    Running setup.py (path:/tmp/pip-req-build-54p4ej83/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-54p4ej83/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-54p4ej83/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-54p4ej83/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-54p4ej83/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-54p4ej83/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-54p4ej83/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-54p4ej83/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-54p4ej83 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-yzg_yeyt'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-dew9w9sm\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-dew9w9sm\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-54p4ej83/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-54p4ej83/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-dew9w9sm --python-tag cp37\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.8.1+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-54p4ej83/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-dew9w9sm/apex-0.1-cp37-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=198540 sha256=9048b18f43e3c66630b81c7f5860b27497a392273a4d75ea4242dd8d7b181770\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ij6ne1gy/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-54p4ej83\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Created temporary directory: /usr/local/lib/python3.7/dist-packages/~pex-0.1.dist-info\n",
            "      Removing file or directory /usr/local/lib/python3.7/dist-packages/apex-0.1.dist-info/\n",
            "      Created temporary directory: /usr/local/lib/python3.7/dist-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.7/dist-packages/apex/\n",
            "      Successfully uninstalled apex-0.1\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-yzg_yeyt'\n",
            "/content\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_8NAkHn2FmE"
      },
      "source": [
        "## Bert Pre-Trained Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvW_wHIe2IEJ"
      },
      "source": [
        "def tokenization(): \n",
        "    # tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "    return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU1I5E0Q2ILU"
      },
      "source": [
        "## Bert Pre-Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz7RCx3I2KHd"
      },
      "source": [
        "def mctest_model():\n",
        "    # config = BertConfig.from_pretrained('bert-large-uncased', num_labels=4)\n",
        "    # model = BertForMultipleChoice.from_pretrained('bert-large-uncased', config=config)\n",
        "    config = AlbertConfig.from_pretrained('albert-base-v2', num_labels=4)\n",
        "    model = AlbertForMultipleChoice.from_pretrained('albert-base-v2', config=config)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HECgRrN7TVV_"
      },
      "source": [
        "## MCTest Features Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adGXPSFPTXpj"
      },
      "source": [
        "class InputMCTestFeatures(object):\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 choices_features,\n",
        "                 label):\n",
        "        self.example_id = example_id\n",
        "        self.choices_features = [\n",
        "            {\n",
        "                'input_ids': input_ids,\n",
        "                'segment_ids': segment_ids,\n",
        "                'input_mask': input_mask\n",
        "            }\n",
        "            for _, input_ids, segment_ids, input_mask in choices_features\n",
        "        ]\n",
        "        self.label = label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nGVCsRqTYmW"
      },
      "source": [
        "## MCTest Object Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkKWs78aTawB"
      },
      "source": [
        "class mctest(object):\n",
        "    def __init__(self,\n",
        "                 test_id,\n",
        "                 article,\n",
        "                 start_ending,\n",
        "                 ending_1,\n",
        "                 ending_2,\n",
        "                 ending_3,\n",
        "                 ending_4,\n",
        "                 label):\n",
        "        self.test_id = test_id\n",
        "        self.article = article\n",
        "        self.start_ending = start_ending\n",
        "        self.endings = [\n",
        "            ending_1,\n",
        "            ending_2,\n",
        "            ending_3,\n",
        "            ending_4,\n",
        "        ]\n",
        "        self.label = label\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        l = [\n",
        "            \"id: {}\".format(self.test_id),\n",
        "            \"article: {}\".format(self.article),\n",
        "            \"question: {}\".format(self.start_ending),\n",
        "            \"option_1: {}\".format(self.endings[0]),\n",
        "            \"option_2: {}\".format(self.endings[1]),\n",
        "            \"option_3: {}\".format(self.endings[2]),\n",
        "            \"option_4: {}\".format(self.endings[3]),\n",
        "        ]\n",
        "\n",
        "\n",
        "        if self.label is not None:\n",
        "            l.append(\"label: {}\".format(self.label))\n",
        "\n",
        "        return \", \".join(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSGQystZudOA"
      },
      "source": [
        "## Prepare MCTest data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh8-xVkuu7oa"
      },
      "source": [
        "def replace_article_str(df):\n",
        "  str_lst = ['\\\\\\\\newline\\\\\\\\newline', '\\\\n', '\\\\', 'newline']\n",
        "  for i in str_lst:\n",
        "    df[\"article\"] = df[\"article\"].str.replace(i, \" \", regex=True)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L4hg3byujKX"
      },
      "source": [
        " def mc_train():\n",
        "   train_lst = []\n",
        "   for i in [\"160\", \"500\"]:\n",
        "     mc_train_question = pd.read_csv(\n",
        "          '/content/MCTest/MCTest/mc{}.train.tsv'.format(i),\n",
        "          sep='\\t',\n",
        "          header=None,\n",
        "          names=[\"id\", \"author-worktimes\", \"article\",\n",
        "                \"q1\", \"q1_c1\", \"q1_c2\", \"q1_c3\", \"q1_c4\",\n",
        "                \"q2\", \"q2_c1\", \"q2_c2\", \"q2_c3\", \"q2_c4\",\n",
        "                \"q3\", \"q3_c1\", \"q3_c2\", \"q3_c3\", \"q3_c4\",\n",
        "                \"q4\", \"q4_c1\", \"q4_c2\", \"q4_c3\", \"q4_c4\", ])\n",
        "     mc_train_ans = pd.read_csv(\n",
        "        '/content/MCTest/MCTest/mc{}.train.ans'.format(i),\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['q1_ans', 'q2_ans', 'q3_ans', 'q4_ans', ])   \n",
        "     # Dropping id and author-worktimes columns\n",
        "     mc_train = mc_train_question.drop(columns=[\"id\", \"author-worktimes\"])\n",
        "     mc_train = mc_dataset_pre(mc_train, mc_train_ans)\n",
        "     mc_train = replace_article_str(mc_train)\n",
        "    #  mc_train['article'] = mc_train['article'].str.replace(\"\\\\n\",\" \")\n",
        "    #  mc_train['article'] = mc_train['article'].str.replace(\"\\\\\\\\newline\",\" \")\n",
        "     train_lst.append(mc_train)\n",
        "   final_train_df = pd.concat(train_lst, ignore_index=True)\n",
        "   return final_train_df\n",
        "\n",
        "def mc_dev():\n",
        "   dev_lst = []\n",
        "   for i in [\"160\", \"500\"]: \n",
        "      mc_dev_question = pd.read_csv(\n",
        "          '/content/MCTest/MCTest/mc{}.dev.tsv'.format(i),\n",
        "          sep='\\t',\n",
        "          header=None,\n",
        "          names=[\"id\", \"author-worktimes\", \"article\",\n",
        "                 \"q1\", \"q1_c1\", \"q1_c2\", \"q1_c3\", \"q1_c4\",\n",
        "                 \"q2\", \"q2_c1\", \"q2_c2\", \"q2_c3\", \"q2_c4\",\n",
        "                 \"q3\", \"q3_c1\", \"q3_c2\", \"q3_c3\", \"q3_c4\",\n",
        "                 \"q4\", \"q4_c1\", \"q4_c2\", \"q4_c3\", \"q4_c4\", ])\n",
        "      mc_dev_ans = pd.read_csv(\n",
        "          '/content/MCTest/MCTest/mc{}.dev.ans'.format(i),\n",
        "          sep='\\t',\n",
        "          header=None,\n",
        "          names=['q1_ans', 'q2_ans', 'q3_ans', 'q4_ans', ])     \n",
        "      # Dropping id and author-worktimes columns\n",
        "      mc_dev = mc_dev_question.drop(columns=[\"id\", \"author-worktimes\"])\n",
        "      mc_dev = mc_dataset_pre(mc_dev, mc_dev_ans)\n",
        "      mc_dev = replace_article_str(mc_dev)\n",
        "      # mc_dev['article'] = mc_dev['article'].str.replace(\"\\\\\",\" \")\n",
        "      # mc_dev['article'] = mc_dev['article'].str.replace(\"\\ newline\",\" \")\n",
        "      dev_lst.append(mc_dev)\n",
        "   final_dev_df = pd.concat(dev_lst, ignore_index=True)\n",
        "   return final_dev_df\n",
        " \n",
        "\n",
        " def mc_test():\n",
        "   test_lst = []\n",
        "   for i in [\"160\", \"500\"]:\n",
        "      mc_test_question = pd.read_csv(\n",
        "          '/content/MCTest/MCTest/mc{}.test.tsv'.format(i),\n",
        "          sep='\\t',\n",
        "          header=None,\n",
        "          names=[\"id\", \"author-worktimes\", \"article\",\n",
        "                 \"q1\", \"q1_c1\", \"q1_c2\", \"q1_c3\", \"q1_c4\",\n",
        "                 \"q2\", \"q2_c1\", \"q2_c2\", \"q2_c3\", \"q2_c4\",\n",
        "                 \"q3\", \"q3_c1\", \"q3_c2\", \"q3_c3\", \"q3_c4\",\n",
        "                 \"q4\", \"q4_c1\", \"q4_c2\", \"q4_c3\", \"q4_c4\", ])\n",
        "      mc_test_ans = pd.read_csv(\n",
        "          '/content/MCTest/MCTestAnswers/mc{}.test.ans'.format(i),\n",
        "          sep='\\t',\n",
        "          header=None,\n",
        "          names=['q1_ans', 'q2_ans', 'q3_ans', 'q4_ans', ])     \n",
        "      # Dropping id and author-worktimes columns\n",
        "      mc_test = mc_test_question.drop(columns=[\"id\", \"author-worktimes\"])\n",
        "      mc_test = mc_dataset_pre(mc_test, mc_test_ans)\n",
        "      mc_test = replace_article_str(mc_test)\n",
        "      # mc_test['article'] = mc_test['article'].str.replace(\"\\n\",\" \")\n",
        "      # mc_test['article'] = mc_test['article'].str.replace(\"\\ newline\",\" \")\n",
        "      test_lst.append(mc_test)\n",
        "   final_test_df = pd.concat(test_lst, ignore_index=True)\n",
        "   return final_test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw4onyQh9lCx"
      },
      "source": [
        "def mc_dataset_pre(df, df_ans):\n",
        "    # Adding new column of the required sentence type of the answer\n",
        "    for i in range(1, 5):\n",
        "        df.insert(loc=df.columns.get_loc(\"q{}\".format(str(i))) + 1,\n",
        "                  column=\"q{}_ans_sen_type\".format(i),\n",
        "                  value=\"\")\n",
        "    # Checking what sentence type is required for finding the answer\n",
        "    for i in range(1, 5):\n",
        "        for k in range(df.shape[0]):\n",
        "            if \"multiple:\" in df.loc[k, \"q{}\".format(str(i))].split()[0]:\n",
        "                df.loc[k, \"q{}_ans_sen_type\".format(i)] = \"multiple\"\n",
        "                df.loc[k, \"q{}\".format(str(i))] = df.loc[k, \"q{}\".format(str(i))].replace(\"multiple:\", \"\")\n",
        "            elif \"one:\" in df.loc[k, \"q{}\".format(str(i))].split()[0]:\n",
        "                df.loc[k, \"q{}_ans_sen_type\".format(i)] = \"one\"\n",
        "                df.loc[k, \"q{}\".format(str(i))] = df.loc[k, \"q{}\".format(str(i))].replace(\"one:\", \"\")\n",
        "\n",
        "    # Adding a new column of the question answer\n",
        "    for i in range(1, 5):\n",
        "        df.insert(loc=df.columns.get_loc(\"q{}_c4\".format(str(i))) + 1,\n",
        "                    column=\"q{}_ans\".format(i),\n",
        "                    value=df_ans.iloc[:, i - 1])\n",
        "\n",
        "    # Adding a new column of the text of the question answer\n",
        "    for i in range(1, 5):\n",
        "        df.insert(loc=df.columns.get_loc(\"q{}_ans\".format(str(i))) + 1,\n",
        "                  column=\"q{}_ans_text\".format(i),\n",
        "                  value=\"\")\n",
        "\n",
        "    # Putting the corresponding answer of the question into the dataframe\n",
        "    for i in range(1, 5):\n",
        "        for k in range(df.shape[0]):\n",
        "            if df.loc[k, \"q{}_ans\".format(str(i))] == \"A\":\n",
        "                df.loc[k, \"q{}_ans_text\".format(str(i))] = df.loc[k, \"q{}_c1\".format(str(i))]\n",
        "            elif df.loc[k, \"q{}_ans\".format(str(i))] == \"B\":\n",
        "                df.loc[k, \"q{}_ans_text\".format(str(i))] = df.loc[k, \"q{}_c2\".format(str(i))]\n",
        "            elif df.loc[k, \"q{}_ans\".format(str(i))] == \"C\":\n",
        "                df.loc[k, \"q{}_ans_text\".format(str(i))] = df.loc[k, \"q{}_c3\".format(str(i))]\n",
        "            elif df.loc[k, \"q{}_ans\".format(str(i))] == \"D\":\n",
        "                df.loc[k, \"q{}_ans_text\".format(str(i))] = df.loc[k, \"q{}_c4\".format(str(i))]\n",
        "\n",
        "    # Get the column of article, question, answer sentence type and answer from training dataset\n",
        "    # Put them into a final training dataset\n",
        "    df_final = pd.DataFrame(columns=[\"article\", \"question\", \"option_1\", \"option_2\", \"option_3\", \"option_4\",\n",
        "                                     \"ans_sen_type\", \"answer\"])\n",
        "    for art in range(df.shape[0]):\n",
        "        for q in range(1, 5):\n",
        "            df_final = df_final.append(pd.DataFrame([[df.loc[art, \"article\"],\n",
        "                                                      df.loc[art, \"q{}\".format(str(q))],\n",
        "                                                      df.loc[art, \"q{}_c1\".format(str(q))],\n",
        "                                                      df.loc[art, \"q{}_c2\".format(str(q))],\n",
        "                                                      df.loc[art, \"q{}_c3\".format(str(q))],\n",
        "                                                      df.loc[art, \"q{}_c4\".format(str(q))],\n",
        "                                                      df.loc[art, \"q{}_ans_sen_type\".format(str(q))],\n",
        "                                                      df.loc[art, \"q{}_ans\".format(str(q))]\n",
        "                                                      # df.loc[art, \"q{}_ans_text\".format(str(q))]\n",
        "                                                      ]],\n",
        "                                                    columns=[\"article\", \"question\", \"option_1\", \"option_2\", \"option_3\",\n",
        "                                                             \"option_4\",\n",
        "                                                             \"ans_sen_type\", \"answer\"]),\n",
        "                                       ignore_index=True)\n",
        "    return df_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li0KVoOju90h"
      },
      "source": [
        "## MCTest Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfZNzUq3vBZl"
      },
      "source": [
        "def mctestDataSet(tokenizer, task_type, max_seq_length):\n",
        "    df = None\n",
        "    if task_type == \"train\":\n",
        "        df = mc_train()\n",
        "    elif task_type == \"dev\":\n",
        "        df = mc_dev()\n",
        "    elif task_type == \"test\":\n",
        "        df = mc_test()\n",
        "  \n",
        "    examples = []\n",
        "    for i in range(df.shape[0]):\n",
        "        answer = ord(df.loc[i, \"answer\"]) - ord('A')\n",
        "        examples.append(mctest(\n",
        "            test_id=\"{}_{}-{}\".format(\"mc\", task_type, i),\n",
        "            article=df.loc[i, \"article\"],\n",
        "            start_ending=df.loc[i, \"question\"],\n",
        "            \n",
        "            ending_1=df.loc[i, \"option_1\"],\n",
        "            ending_2=df.loc[i, \"option_2\"],\n",
        "            ending_3=df.loc[i, \"option_3\"],\n",
        "            ending_4=df.loc[i, \"option_4\"],\n",
        "            label=answer,\n",
        "        ))\n",
        "    features = []\n",
        "    for example_index, example in enumerate(examples):\n",
        "        # article tokens\n",
        "        article_tokens = tokenizer.tokenize(example.article)\n",
        "        # quesiton tokens\n",
        "        start_ending_tokens = tokenizer.tokenize(example.start_ending)\n",
        "\n",
        "        choices_features = []\n",
        "        for ending_index, ending in enumerate(example.endings):\n",
        "            article_tokens_choice = article_tokens[:]\n",
        "            ending_tokens = start_ending_tokens + [\"[SEP]\"] + tokenizer.tokenize(ending)\n",
        "\n",
        "            encode = tokenizer.encode_plus(article_tokens_choice, \n",
        "                                           ending_tokens,\n",
        "                                           add_special_tokens=True,\n",
        "                                           padding='max_length',\n",
        "                                           truncation=True,\n",
        "                                           max_length=max_seq_length)\n",
        "            \n",
        "            input_ids = encode[\"input_ids\"]\n",
        "            segments_ids = encode[\"token_type_ids\"]\n",
        "            input_mask = encode[\"attention_mask\"]\n",
        "            tokens = tokenizer.decode(input_ids)\n",
        "            # print(tokens)\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segments_ids) == max_seq_length\n",
        "\n",
        "            choices_features.append((tokens, input_ids, segments_ids, input_mask))\n",
        "\n",
        "        label = example.label\n",
        "        features.append(InputMCTestFeatures(\n",
        "            example_id=example.test_id,\n",
        "            choices_features=choices_features,\n",
        "            label=label\n",
        "        ))\n",
        "\n",
        "    tensor_input_ids = torch.tensor([[choice[\"input_ids\"] for choice in feature.choices_features] for feature in features], dtype=torch.long)\n",
        "    tensor_segment_ids = torch.tensor([[choice[\"segment_ids\"] for choice in feature.choices_features] for feature in features], dtype=torch.long)\n",
        "    tensor_input_mask = torch.tensor([[choice[\"input_mask\"] for choice in feature.choices_features] for feature in features], dtype=torch.long)\n",
        "    tensor_labels = torch.tensor([f.label for f in features], dtype=torch.long)   \n",
        "    tensor_dataset = TensorDataset(tensor_input_ids, tensor_segment_ids, tensor_input_mask, tensor_labels)  \n",
        "      \n",
        "    if task_type == \"test\":\n",
        "      new_test_data = []\n",
        "      for i in tensor_dataset:\n",
        "        new_input_ids = i[0]\n",
        "        new_segment_ids = i[1]\n",
        "        new_input_masks = i[2]\n",
        "        # new_labels = None\n",
        "        # new_test_data.append([new_input_ids, new_segment_ids, new_input_masks, new_labels])\n",
        "        new_test_data.append([new_input_ids, new_segment_ids, new_input_masks])\n",
        "      tensor_dataset = tuple(new_test_data)\n",
        "\n",
        "    return tensor_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPa0vHp1b61v"
      },
      "source": [
        "## Defining GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1eHhAlub6c8"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saxztXcC7plN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5900996c-f51d-4948-ba67-eb2b9305fd2f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr 10 10:51:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0    31W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bW5JHjMXxUo"
      },
      "source": [
        "## Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGL2tpeIXyXZ"
      },
      "source": [
        "def get_predictions(model, dataloader, compute_acc=False):\n",
        "    predictions = None\n",
        "    correct = 0\n",
        "    total = 0\n",
        "  \n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            if next(model.parameters()).is_cuda:\n",
        "                data = [t.to(device) for t in data if t is not None]\n",
        "            \n",
        "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
        "            outputs = model(input_ids=tokens_tensors, \n",
        "                            token_type_ids=segments_tensors, \n",
        "                            attention_mask=masks_tensors)\n",
        "            \n",
        "            # print(outputs, \"\\n\")\n",
        "            logits = outputs[0]\n",
        "            # print(\"logits: {}\".format(logits))\n",
        "                        \n",
        "            _, pred = torch.max(logits.data, 1)\n",
        "            \n",
        "            if compute_acc:\n",
        "                labels = data[3]\n",
        "                total += labels.size(0)\n",
        "                correct += (pred == labels).sum().item()\n",
        "                \n",
        "            if predictions is None:\n",
        "                predictions = pred\n",
        "            else:\n",
        "                predictions = torch.cat((predictions, pred))\n",
        "    \n",
        "    if compute_acc:\n",
        "        acc = correct / total\n",
        "        return predictions, acc\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23IQWPDfaXs6"
      },
      "source": [
        "## Run MCTest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WpbZmcBauOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d32df8f6-d09e-42e0-dca6-47edabcfb643"
      },
      "source": [
        "BATCH = 8\n",
        "MAX_SEQ_LEN = 128\n",
        "LR = 2e-5\n",
        "\n",
        "tokenizer = tokenization()\n",
        "model = mctest_model()\n",
        "model = model.to(device)\n",
        "\n",
        "train_data = mctestDataSet(tokenizer, \"train\", MAX_SEQ_LEN)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH)\n",
        "  \n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "adam_optimizer = AdamW(optimizer_grouped_parameters, lr=LR)\n",
        "model, adam_optimizer = amp.initialize(model, adam_optimizer, opt_level=\"O1\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMultipleChoice: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
            "- This IS expected if you are initializing AlbertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForMultipleChoice were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUfl0SaNwBEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e052e0-bf94-4469-c3cf-68f661354a01"
      },
      "source": [
        "EPOCHS = 3\n",
        "print(\"Start Training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    train_loss = 0.0\n",
        "    t0_epoch, t0_batch = time.time(), time.time()\n",
        "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "    model.train()\n",
        "    # for data in train_dataloader:\n",
        "    for data in train_dataloader:\n",
        "      tokens_tensors, segments_tensors, masks_tensors, labels_tensor = [t.to(device) for t in data]\n",
        "      \n",
        "      # Set the gradient to 0\n",
        "      adam_optimizer.zero_grad()\n",
        "      outputs = model(input_ids=tokens_tensors, \n",
        "                        token_type_ids=segments_tensors, \n",
        "                        attention_mask=masks_tensors, \n",
        "                        labels=labels_tensor)\n",
        "\n",
        "      loss = outputs[0]\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      with amp.scale_loss(loss, adam_optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "      adam_optimizer.step()\n",
        "\n",
        "    # Calculating Accuracy\n",
        "    _, train_accuracy = get_predictions(model, train_dataloader, compute_acc=True)\n",
        "\n",
        "    ## Evaluation\n",
        "    model.eval()    \n",
        "    tokenizer = tokenization()\n",
        "    dev_data = mctestDataSet(tokenizer=tokenizer, task_type=\"dev\", max_seq_length=MAX_SEQ_LEN)\n",
        "    dev_dataloader = DataLoader(dev_data, batch_size=BATCH)   \n",
        "    # Tracking variables\n",
        "    dev_accuracy = []\n",
        "    dev_loss = []   \n",
        "    eval_time_start = time.time()\n",
        "    # For each batch in our validation set...\n",
        "    for data in dev_dataloader:\n",
        "        # Load batch to GPU\n",
        "        tokens_tensors, segments_tensors, masks_tensors, labels_tensor = [t.to(device) for t in data]   \n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "          outputs = model(input_ids=tokens_tensors, \n",
        "                      token_type_ids=segments_tensors, \n",
        "                      attention_mask=masks_tensors, \n",
        "                      labels=labels_tensor)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        # Compute loss\n",
        "        dev_loss.append(loss.item())    \n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()   \n",
        "        # Calculating Accuracy\n",
        "\n",
        "    _, dev_acc = get_predictions(model, dev_dataloader, compute_acc=True)\n",
        "    dev_accuracy.append(dev_acc)\n",
        "    eval_time_stop = time.time()    \n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    dev_loss = np.mean(dev_loss)\n",
        "    dev_accuracy = np.mean(dev_accuracy)\n",
        "    \n",
        "    epoch_end = time.time()   \n",
        "    print(\"Training Epoch: {}, TRAIN-Loss: {}, TRAIN-Accuracy: {}, DEV-Loss: {}, DEV-Accuracy: {},Epcoh Time: {}\".\n",
        "          format(epoch+1, train_loss, train_accuracy, dev_loss, dev_accuracy, epoch_end-epoch_start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training...\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Training Epoch: 1, TRAIN-Loss: 227.33915901184082, TRAIN-Accuracy: 0.6297297297297297, DEV-Loss: 1.031173864006996, DEV-Accuracy: 0.571875,Epcoh Time: 126.07585573196411\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Training Epoch: 2, TRAIN-Loss: 165.5555994808674, TRAIN-Accuracy: 0.7155405405405405, DEV-Loss: 0.9813106685876847, DEV-Accuracy: 0.571875,Epcoh Time: 125.93611097335815\n",
            "Training Epoch: 3, TRAIN-Loss: 120.9088600948453, TRAIN-Accuracy: 0.8648648648648649, DEV-Loss: 0.9670368945226073, DEV-Accuracy: 0.609375,Epcoh Time: 126.0745460987091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438UXtsHipd5"
      },
      "source": [
        "## Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5-PMX9UipHD"
      },
      "source": [
        "torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftFGFnalitfI"
      },
      "source": [
        "## Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gd34sePitC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7861b52-ba04-4f66-d374-d985bc19bea9"
      },
      "source": [
        "state_dict = torch.load('checkpoint.pth')\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['albert.embeddings.position_ids', 'albert.embeddings.word_embeddings.weight', 'albert.embeddings.position_embeddings.weight', 'albert.embeddings.token_type_embeddings.weight', 'albert.embeddings.LayerNorm.weight', 'albert.embeddings.LayerNorm.bias', 'albert.encoder.embedding_hidden_mapping_in.weight', 'albert.encoder.embedding_hidden_mapping_in.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'albert.pooler.weight', 'albert.pooler.bias', 'classifier.weight', 'classifier.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeHScujBizfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008adf49-d1b6-4a3f-e044-dacdbd25d660"
      },
      "source": [
        "model = mctest_model()\n",
        "model = model.to(device)\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMultipleChoice: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
            "- This IS expected if you are initializing AlbertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForMultipleChoice were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rXoSp3GnKb-"
      },
      "source": [
        "## Test Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmQCEkMEp4xL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb37778-2a3b-4097-ad53-803bc2a6eae1"
      },
      "source": [
        "TEST_MAX_SEQ_LEN = 128\n",
        "TEST_BATCH = 8\n",
        "\n",
        "model.eval()\n",
        "tokenizer = tokenization()\n",
        "\n",
        "test_data = mctestDataSet(tokenizer=tokenizer, task_type=\"test\", max_seq_length=TEST_MAX_SEQ_LEN)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=TEST_BATCH)\n",
        "\n",
        "predictions = get_predictions(model, test_dataloader)\n",
        "\n",
        "# Testing Result Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "label_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
        "index_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "df = pd.DataFrame({\"pre_answer\": predictions.tolist()})\n",
        "df['pre_answer'] = df.pre_answer.apply(lambda x: index_map[x])\n",
        "df_pred = pd.concat([mc_test().loc[:, [\"article\"]], \n",
        "                     mc_test().loc[:, [\"question\"]], \n",
        "                     mc_test().loc[:, [\"answer\"]], \n",
        "                     df.loc[:, 'pre_answer']], axis=1)\n",
        "\n",
        "list_true = df_pred.loc[:, [\"answer\"]].values.tolist()\n",
        "list_pre = df_pred.loc[:, [\"pre_answer\"]].values.tolist()\n",
        "\n",
        "print(\"Testing Accuracy: {}\".format(accuracy_score(list_true, list_pre)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.6023809523809524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdEFvCGtpLDz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9064297e-0f7a-489c-c59a-886ac7baa999"
      },
      "source": [
        "df_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>pre_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Todd is a small boy in the town of Rocksville....</td>\n",
              "      <td>How long was it before Todd made it to the rock?</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Todd is a small boy in the town of Rocksville....</td>\n",
              "      <td>What did Todd have to do so his dad would wat...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Todd is a small boy in the town of Rocksville....</td>\n",
              "      <td>What was Todd's favorite part of Lake Keet?</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Todd is a small boy in the town of Rocksville....</td>\n",
              "      <td>How did Todd's dad celebrate Todd's hard work?</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mortamer was a tree monkey who lived in the ju...</td>\n",
              "      <td>What scared Mortamer?</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>Greg and his mother were building a racing car...</td>\n",
              "      <td>Where was the race happening?</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>Joey went to a baseball game during the winter...</td>\n",
              "      <td>Who went to the baseball game and with how ma...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837</th>\n",
              "      <td>Joey went to a baseball game during the winter...</td>\n",
              "      <td>what kind of store did Joey turn into?</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>Joey went to a baseball game during the winter...</td>\n",
              "      <td>Which team won the game Joey went to and by h...</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>839</th>\n",
              "      <td>Joey went to a baseball game during the winter...</td>\n",
              "      <td>What dessert did Joey choose?</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>840 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               article  ... pre_answer\n",
              "0    Todd is a small boy in the town of Rocksville....  ...          C\n",
              "1    Todd is a small boy in the town of Rocksville....  ...          D\n",
              "2    Todd is a small boy in the town of Rocksville....  ...          B\n",
              "3    Todd is a small boy in the town of Rocksville....  ...          A\n",
              "4    Mortamer was a tree monkey who lived in the ju...  ...          D\n",
              "..                                                 ...  ...        ...\n",
              "835  Greg and his mother were building a racing car...  ...          B\n",
              "836  Joey went to a baseball game during the winter...  ...          D\n",
              "837  Joey went to a baseball game during the winter...  ...          A\n",
              "838  Joey went to a baseball game during the winter...  ...          A\n",
              "839  Joey went to a baseball game during the winter...  ...          D\n",
              "\n",
              "[840 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}